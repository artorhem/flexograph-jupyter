{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "datasets = [\"road_asia\"] #, \"orkut\", \"road_usa\", \"livejournal\", \"dota_league\", \"graph500_23\", \"graph500_26\", \"graph500_28\", \"graph500_30\"]\n",
    "RESULTS_DIR = \"/Users/Puneet89/scratch/flexograph-eval/results\"\n",
    "DATASET_DIR = \"/Users/Puneet89/scratch/flexograph-eval/datasets\"\n",
    "\n",
    "#we now need to construct a dataframe with the following columns:\n",
    "#dataset, benchmark, system, preprocessing_time,exec_time\n",
    "\n",
    "df_results = pd.DataFrame(columns=['dataset', 'benchmark', 'system', 'preprocessing_time', 'exec_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAPBS\n",
    "All the datasets are basically just dumps of the output from running the commands and we must first parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset benchmark system  preprocessing_time  exec_time\n",
      "0  road_asia        pr  gapbs             1.38748    0.16379\n",
      "1  road_asia       bfs  gapbs             1.37387    0.01518\n",
      "2  road_asia        cc  gapbs             1.46481    0.01272\n",
      "3  road_asia      sssp  gapbs             1.47599    0.05406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/prv9ywh52sn57l626bl42pz40000gp/T/ipykernel_19758/3450089767.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "SYSTEM=\"gapbs\"\n",
    "benchmarks = [\"pr\", \"bfs\", \"cc\", \"sssp\"]\n",
    "for dataset in datasets:\n",
    "  for benchmark in benchmarks:\n",
    "    #check if the file exists\n",
    "    if os.path.exists(f\"{RESULTS_DIR}/{SYSTEM}/{dataset}_{benchmark}.txt\"):\n",
    "      #open the file and read the contents\n",
    "      with open(f\"{RESULTS_DIR}/{SYSTEM}/{dataset}_{benchmark}.txt\" , 'r') as f:\n",
    "        regex = r\"^(Read|Build|Trial)\\sTime:\\s+(\\d+\\.\\d+)\"\n",
    "        input = f.read()\n",
    "        # Search for all occurrences of the regex pattern\n",
    "        matches = re.finditer(regex, input, re.MULTILINE)\n",
    "\n",
    "        # Print the matches\n",
    "        read_times = [] \n",
    "        build_times = []\n",
    "        trial_times = []\n",
    "        for match in matches:\n",
    "            #we want to print the sum of times if the first element of the tuple is 'Read' or 'Build'\n",
    "            if match.group(1) == 'Read':\n",
    "                read_times.append(float(match.group(2)))\n",
    "            elif match.group(1) == 'Build':\n",
    "                build_times.append(float(match.group(2)))\n",
    "            else:\n",
    "                trial_times.append(float(match.group(2)))\n",
    "        preprocessing_time = round(sum(read_times)/len(read_times) + sum(build_times)/len(build_times),5)\n",
    "        exec_time = round(sum(trial_times)/len(trial_times),5)\n",
    "        new_tuple = (f\"{dataset}\", f\"{benchmark}\", f\"{SYSTEM}\", preprocessing_time,exec_time)\n",
    "        # Convert the tuple into a DataFrame with the same column names\n",
    "        new_row = pd.DataFrame([new_tuple], columns=df_results.columns)\n",
    "        # Use pd.concat to append the new row\n",
    "        df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
    "\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.848687\n",
      "  dataset_name benchmark_name  cores  threads   time(s)\n",
      "0    road_asia       pagerank    192        2  0.768005\n",
      "1    road_asia       pagerank    192        2  0.530857\n",
      "2    road_asia       pagerank    192        2  0.540689\n",
      "3    road_asia       pagerank    192        2  0.533989\n",
      "4    road_asia       pagerank    192        2  0.544544\n",
      "  dataset_name benchmark_name  cores  threads   time(s)\n",
      "0    road_asia            bfs    192        2  0.004874\n",
      "1    road_asia            bfs    192        2  0.002845\n",
      "2    road_asia            bfs    192        2  0.002986\n",
      "3    road_asia            bfs    192        2  0.002948\n",
      "4    road_asia            bfs    192        2  0.006239\n",
      "  dataset_name benchmark_name  cores  threads   time(s)\n",
      "0    road_asia             cc    192        2  0.346421\n",
      "1    road_asia             cc    192        2  0.076548\n",
      "2    road_asia             cc    192        2  0.074080\n",
      "3    road_asia             cc    192        2  0.073958\n",
      "4    road_asia             cc    192        2  0.074142\n",
      "  dataset_name benchmark_name  cores  threads   time(s)\n",
      "0    road_asia           sssp    192        2  0.004555\n",
      "1    road_asia           sssp    192        2  0.003310\n",
      "2    road_asia           sssp    192        2  0.002901\n",
      "3    road_asia           sssp    192        2  0.002830\n",
      "4    road_asia           sssp    192        2  0.005837\n"
     ]
    }
   ],
   "source": [
    "#we first need to find the preprocessing time by reading the file datasets/{dataset}/{dataset}_gemini_convert.log\n",
    "\n",
    "SYSTEM=\"gemini\"\n",
    "\n",
    "benchmarks = [\"pagerank\", \"bfs\", \"cc\", \"sssp\"]\n",
    "for dataset in datasets:\n",
    "  preprocessing_time = 0\n",
    "  with open(f\"{DATASET_DIR}/{dataset}/{dataset}_gemini_convert.log\" , 'r') as f:\n",
    "    regex = r\"^time:\\s+(\\d+\\.\\d+)\"\n",
    "    input = f.read()\n",
    "    # Search for all occurrences of the regex pattern\n",
    "    matches = re.finditer(regex, input, re.MULTILINE)\n",
    "    for match in matches:\n",
    "      preprocessing_time = float(match.group(1))\n",
    "      print(preprocessing_time)\n",
    "  for benchmark in benchmarks:\n",
    "    df = pd.read_csv(f\"{RESULTS_DIR}/{SYSTEM}/{dataset}_{benchmark}.csv\", sep=\",\",names=[\"dataset_name\", \"benchmark_name\", \"cores\", \"threads\", \"time(s)\"],header=0)\n",
    "    print(df.head())\n",
    "    exec_time = round(df[\"time(s)\"].mean(),5)\n",
    "    new_tuple = (f\"{dataset}\", f\"{benchmark}\", f\"{SYSTEM}\", preprocessing_time,exec_time)\n",
    "    new_row = pd.DataFrame([new_tuple], columns=df_results.columns)\n",
    "    df_results = pd.concat([df_results, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset benchmark  system  preprocessing_time  exec_time\n",
      "0  road_asia  pagerank  gemini           13.848687    0.58191\n",
      "1  road_asia       bfs  gemini           13.848687    0.00490\n",
      "2  road_asia        cc  gemini           13.848687    0.12075\n",
      "3  road_asia      sssp  gemini           13.848687    0.00388\n"
     ]
    }
   ],
   "source": [
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset benchmark  system  preprocessing_time  exec_time\n",
      "0  road_asia        pr   gapbs            1.387480    0.16379\n",
      "1  road_asia       bfs   gapbs            1.373870    0.01518\n",
      "2  road_asia        cc   gapbs            1.464810    0.01272\n",
      "3  road_asia      sssp   gapbs            1.475990    0.05406\n",
      "4  road_asia  pagerank  gemini           13.848687    0.58852\n"
     ]
    }
   ],
   "source": [
    "#we first need to find the preprocessing time by reading the file datasets/{dataset}/{dataset}_gemini_convert.log\n",
    "\n",
    "SYSTEM=\"ligra\"\n",
    "\n",
    "benchmarks = [\"PageRank\", \"BFS\", \"Components\", \"Triangle\"]\n",
    "def change_benchmark_name(benchmark):\n",
    "    if(benchmark == \"PageRank\"):\n",
    "      benchmark = \"pr\"\n",
    "    elif(benchmark == \"BFS\"):\n",
    "      benchmark = \"bfs\"\n",
    "    elif(benchmark == \"Components\"):\n",
    "      benchmark = \"cc\"\n",
    "    elif(benchmark == \"Triangle\"):\n",
    "      benchmark = \"tc\"\n",
    "    return benchmark\n",
    "\n",
    "for dataset in datasets:\n",
    "  for benchmark in benchmarks:\n",
    "    preprocessing_time = 0\n",
    "    exec_times=[]\n",
    "    with open(f\"{RESULTS_DIR}/{SYSTEM}/{dataset}_{benchmark}.txt\" , 'r') as f:\n",
    "      regex_preprocessing = r\"^Time[\\s+\\w+]*:\\s+(\\d+.\\d+)\\s+seconds\"\n",
    "      regex_exectime=r\"^Running time :\\s+(\\d+.\\d+)\"\n",
    "      input = f.read()\n",
    "      # Search for all occurrences of the regex pattern\n",
    "      matches = re.finditer(regex_preprocessing, input, re.MULTILINE)\n",
    "      for match in matches:\n",
    "        preprocessing_time = float(match.group(1))\n",
    "      matches = re.finditer(regex_exectime, input, re.MULTILINE)\n",
    "      for match in matches:\n",
    "        exec_times.append(float(match.group(1)))\n",
    "      exec_time = round(sum(exec_times)/len(exec_times),5)\n",
    "      benchmark = change_benchmark_name(benchmark)\n",
    "      new_tuple = (f\"{dataset}\", f\"{benchmark}\", f\"{SYSTEM}\", preprocessing_time,exec_time)\n",
    "      new_row = pd.DataFrame([new_tuple], columns=df_results.columns)\n",
    "      df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
    "  \n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galois \n",
    "I think we should only plot the results for the algorithms they say work best in the README\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM=\"galois\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
